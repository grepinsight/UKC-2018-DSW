Intermediate R
========================================================
author: Albert Lee, Stephen Park
date: August 3-4, 2018
autosize: true
transition: none
incremental: true


<style>
.small-code pre code {
  font-size: 1em;
}
</style>

Agenda
========================================================
0. Rmarkdown introduction
1. Problem Statement
2. Exploratory Analysis
3. Machine Learning with Caret
4. K-means Clustering in R

```{r echo=FALSE}
library(caret)
library(tidyverse)
library(useful)
library(cluster)
knitr::opts_chunk$set(echo = TRUE, fig.align="center") # center the figure

```


R Markdown Notebook
===================================
type: section


R-markdown Notebook
========================================================

* R markdown notebook is a literate programming environment for R.
* It's nice because you can
	* keep all your outputs in one place
	* make your analysis reproducible
	* compile into other formats such as `html` or `pdf`
* We will do all the exercises in R markdown notebook

> Open ./exercises/intermediate/exercise.Rmd

Setup
===================================
```{r eval=FALSE}
# Download
download.file("https://tinyurl.com/ukc-r-exercise", "exercise.Rmd")
```

Iris Dataset
===================================
type: section


Motivation
========================================================

* `iris` is a famous dataset put together by the father of statistics, RA Fisher.
* We will use this dataset to practice data analysis
* Go to Exercise 1 P-001

![](assets/iris-machinelearning.png)

<small>
source: Fisher,R.A. "The use of multiple measurements in taxonomic problems" (see [here](http://archive.ics.uci.edu/ml/datasets/Iris) for more info)
</small>

Visualization
===================================
type: section


Visualization with ggplot2
===========================

- Good exploratory analysis lends itself to visualization
- `ggplot2` is THE package for the job
- with `ggplot`, you can make complex plots with just a few lines of code
- based on the theory __g__rammar of __g__raphics, hence the short name `gg`plot.

Visualization with ggplot2
===========================

Every ggplot has three parts

1. __data__ (usually in the form of `data.frame`)
2. __aesthetic mapping__ between variables in the data and the visual properties
	* examples include `x`, `y`, `color`, `share`, `group`, `size`, etc.
3. __layer__ rendered by one of geom_* functions


ggplot2 - (1) The Basics
===========================

![](assets/ggplot_slide1.png)

ggplot2 - (1) The Basics
===========================

![](assets/ggplot_slide2.png)

ggplot2 - (1) The Basics
===========================

![](assets/ggplot_slide3.png)

ggplot2 - (1) The Basics
===========================

![](assets/ggplot_slide4.png)


Examples of Geoms
===========================

* __geom_point()__ produces a scatterplot.
* __geom_smooth()__ fits a smoother to the data and displays the smooth and its
standard error.
* __geom boxplot()__ produces a box-and-whisker plot to summarise the distribution
of a set of points.
* __geom histogram()__ and geom freqpoly() show the distribution of continuous
variables.
• __geom_col()__ shows the distribution of categorical variables.
• __geom_line()__ draw lines between the data points.

ggplot2 - (2) Layers
===========================
incremental: false
class: small-code

```{r }
ggplot(iris, aes(x=Sepal.Width, y=Sepal.Length, color=Species)) +
  geom_point() +
  geom_smooth() # Adding scatterplot geom (layer1) and smoothing geom (layer2).
```

ggplot2 - (2) Layers
===========================
incremental: false
class: small-code
```{r}
ggplot(iris) +
  geom_point(aes(x=Sepal.Width, y=Sepal.Length, shape=Species)) +
  geom_smooth(aes(x=Sepal.Width, y=Sepal.Length, color=Species)) # Same as above but specifying the aesthetics inside the geoms.
```

ggplot2 - (3)  Labels
===========================
incremental: false
class: small-code

```{r}
gg <- ggplot(iris, aes(x=Sepal.Width, y=Sepal.Length, color=Species)) +
  geom_point() +
  labs(title="Scatterplot", x="Sepal Width", y="Sepal Length")  # add axis lables and plot title.
gg
```

ggplot2 - (4) The Facets
===========================
class: small-code
incremental: false

There is a technique called `faceting` to display additional categorical variables:

```{r}
gg +
  facet_wrap( ~ Species, ncol=3)  # columns defined by 'Species'
```

Note that the name of variable is preceded by `~`.

Numerous Possibilites in ggplot2...
===================================
class: small-code
incremental: false

```{r}
ggplot(mtcars, aes(x=cyl)) + geom_bar(fill='darkgoldenrod2') +
  theme(panel.background = element_rect(fill = 'steelblue')) +
  theme(panel.grid.major = element_line(colour = "firebrick", size=3)) +
  theme(panel.grid.minor = element_line(colour = "blue", size=1))
```

ggplot2 Documentation
===================================
incremental: false

[Link](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf)

```{r fig.width=10, fig.height=8,echo=FALSE}
library(png)
library(grid)
img <- readPNG("assets/ggplot2-cheatsheet.png")
grid.raster(img)

```

Visualization Exercise
===================================

* Go to the section `Exercise 2(P-002)` on the notebook and let's do the problems together


Machine Learning
===================================
type: section

Machine Learning
=======================================

What is ML?

> INPUT -> FUNCTION -> OUTPUT

Basically, given `INPUT` and `OUTPUT`, What is FUNCTION?

Two Types of ML: Supervised vs. Unsupervised Learning
=======================================

```{r echo=FALSE, out.width = "1400px"}
knitr::include_graphics("assets/Supervised-Unsupervised.jpg")
```

Classification Problem
==============================================
class:small-code

__Question: can we guess the type of species based on 4 features?__

```{r echo=FALSE}
iris %>%
  group_by(Species) %>%
  sample_n(3) %>%
  ungroup() %>%
  mutate(Species="???")
```

There are more than one ways to solve this problem, but we will use a particular way to do this....

Machine Learning Workflow
=========================


![](assets/machine_learning_workflow.png)

1. Why do we split a data into train/test datasets?


What is caret?
==============

* It stands for  __C__lassification __A__nd __RE__gression __T__raining
* Machine learning framework in R
* Learn one workflow, leverage +100 models
* http://caret.r-forge.r-project.org/

```{r}
# type `help(package = "caret")` for more info
```

Caret consists of 4 steps
=========================

1. Partition data into train / test data
1. Model Selection and Parameter tuning
1. Prediction
1. Performance check


Training/Test Split
===================

```{r}
set.seed(123)
idx_train <- caret::createDataPartition(
  y = iris$Species,
  p = .75,
  list = FALSE
)
```

1. What does `set.seed(123)` do?
2. Read the documentation of `caret::createDataPartition`
3. what does `p=0.75` mean?
4. can you `str(idx_train)` to see its structure?

Training/Test Split
===================

```{r}
str(idx_train)
class(idx_train)
```


Training/Test Split
===================
```{r}
head(idx_train)
```

Training/Test Split Cont
===================
```{r}
data_all <- iris
data_training <- data_all[idx_train, ]
data_test <- data_all[-idx_train, ]
```

1. what am I doing here?
2. what does `-idx_train` mean?
3. can you run `dim` commands on each of the dataset?

```{r eval=FALSE}
dim(data_all)
dim(data_training)
dim(data_test)
```

Model Building - Choosing a model
=================================
incremental: false

* There are many off-the-shelf classification models
	1. random forest
	2. K-nearest neighbor
	3. logistic regression
	4. and more


Model Building - Choosing a model
=================================
incremental: false


* Each model has one or more tuning parameters!


|label                     |model   |params                                                                        |
|:-------------------------|:-------|:-----------------------------------------------------------------------------|
|Random Forest             |rf      |mtry                                                                          |
|k-Nearest Neighbors       |knn     |k                                                                             |
|glmnet                    |glmnet  |alpha, lambda                                                                 |
|eXtreme Gradient Boosting |xgbTree |nrounds, max_depth, eta, gamma, colsample_bytree, min_child_weight, subsample |

* We will use random forest for this session

Model Building - Random Forest, quickstart
=================================
class: small-code

```{r eval=FALSE}
# train model
model_rf <- caret::train(Species ~ . ,
                         data=data_training,
                         method='rf',
                         trControl=trainControl(method="none"),
                         tuneGrid=data.frame(mtry=2)
                         )
# predict
data_pred <- predict(model_rf, newdata=data_test)

# performance check
caret::confusionMatrix(data_pred, data_test$Species)
```

Model Building - Random Forest, problem
=================================

* For `rf`, `mtry` is number of variables randomly sampled as candidates at each split
* previously, we just choose an arbitrary one, namely 2.
* But this approach is problematic because we might not have chosen the best `mtry`.

Model Building - Cross Validation
===================
incremental: false

* Cross-validation is a procedure in which we use different subsets of the data to get multiple measures of model quality.
* We use CV to tune hyperparameters such as `mtry`
* `caret`'s got a function for that!

![](assets/cross_validation.png)

Model Building - Cross Validation in caret
===================


```{r}
ctrl <- caret::trainControl(
  method = "cv",
  number = 5,
)

model_rf2 <- caret::train(
  x = data_training[,1:4],
  y = data_training$Species,
  trControl = ctrl,
  tuneGrid=data.frame(mtry=c(1,2,3,4)),
  method = "rf",
)
```


Model Building
===================
incremental: false

```{r}
ggplot(model_rf2)
```


Prediction
==========
```{r}
data_pred_class <- predict(model_rf2, newdata=data_test)
```

Prediction
==========

```{r eval=FALSE}
confusionMatrix(data = data_pred_class, data_test$Species)
```

1. what is the sensitivity?
2. what is the accuracy?


Variable Importance
====================

```{r eval=TRUE}
caret::varImp(model_rf2)
```

1. What variables are important?
2. Can you confirm this with a plot?

Variable Importance
====================
incremental: false

```{r}
ggplot(iris, aes(x=Petal.Width, fill=Species)) +
  geom_density() +
  theme_bw() +
  labs(title="Petal Width Distribution by Species")
```

Excercise
==========================

Let's do the exercise together


More sophisticated version
==========================

```{r}
model_v2 <- caret::train(
  x = data_training[,1:4],
  y = data_training$Species,
  trControl = ctrl,
  method = "rf",
  ## Center and scale the predictors for the training
  ## set and all future samples.
  preProc = c("center", "scale"),
  metric="logLoss"
)
```


Tips
====

```{r}
info_caret_models  <- caret::getModelInfo()
head(names(info_caret_models))
```


You can do Regression with caret as well
===========
```{r }
ctrl <- caret::trainControl(method="cv", number = 10)
model_linear <- caret::train(x = data_training[,1:3],
             y = data_training$Petal.Width,
             method = "lm",
             trControl = ctrl,
             metric="RMSE")
caret::RMSE(predict(model_linear, newdata=data_test), data_test$Petal.Width)
```

Unsupervised Machine Learning: Clustering
===================================
type: section

Unsupervised Machine Learning: Clustering
=========================================
```{r echo=FALSE, out.width = "1400px"}
knitr::include_graphics("assets/kmeans.jpg")
```

K-means Clustering of the Iris Data
========================================================
```{r}
set.seed(278613)
iris_without_class <- iris[, which(names(iris) != "Species")]
results <- kmeans(x = iris_without_class, centers = 3)
head(results, n=2)
```

Base Scatterplots for the Petals
========================================================

```{r, fig.height=10, fig.width=19}
par(mfrow=c(1,2), bty="n", cex=2)
plot(iris[c("Petal.Length", "Petal.Width")], col=results$cluster)
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)
```


Base Scatterplots for the Sepals
========================================================

```{r, fig.height=10, fig.width=19}
par(mfrow=c(1,2), bty="n", cex=2)
plot(iris[c("Sepal.Length", "Sepal.Width")], col=results$cluster)
plot(iris[c("Sepal.Length", "Sepal.Width")], col=iris$Species)
```

Analyzing the Clustering Results
========================================================

```{r}
table(iris$Species, results$cluster)
```

1. Which Iris plant is linearly separable from the other?

3. Does the results of the K-means clustering show “the data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant”?

Fit Assessment of K-means Clustering
========================================================

```{r}
plot(table(iris$Species, results$cluster), main="Confustion Matrix for Iris Clustering", "xlab=Cultivar", ylab="Cluster")
```

Choosing the Right Number of Clusters: Hartigan's Rule
========================================================
```{r, fig.height=5, fig.width=10}
irisBest <- FitKMeans(iris_without_class, max.clusters=20, nstart=25)
PlotHartigan(irisBest)
```

Another Method: the Gap Statistic
========================================================
```{r, fig.height=5, fig.width=10}
theGap <- clusGap(iris_without_class, FUNcluster=pam, K.max=20)
gapDF <- as.data.frame(theGap$Tab)
ggplot(gapDF, aes(x=1:nrow(gapDF))) + geom_line(aes(y=gap))
```

Futher studies
==============

1. [Caret Documentation](https://topepo.github.io/caret/index.html)
2. [R for Data Science](http://r4ds.had.co.nz/)
3. ggplot2 - Elegant Graphics for Data Analysis by Hadley Wickham


Slides
===================================
* Slide link: <https://grepinsight.github.io/UKC-2018-DSW>
* Github repo: <https://bit.ly/ukc2018-dsw-r>

